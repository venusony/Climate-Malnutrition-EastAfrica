{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "adffd224-1292-4f04-a0c0-7d42bc803702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i want one clean pass that only renames and exports (no dropping)\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "OVERWRITE = False  # if files already exist, skip them. set True only if i need to redo.\n",
    "\n",
    "base = Path(r\"C:\\Users\\venus\\OneDrive\\Documents\\GitHub\\Climate-Malnutrition-EastAfrica\")\n",
    "unzipped = base / \"data_raw\" / \"unzipped\"          # where my renamed .dta files live\n",
    "std_dir  = base / \"data_cleaned\" / \"standardized\"  # parquet outputs\n",
    "csv_dir  = std_dir / \"csv_exports\"                 # csv outputs\n",
    "\n",
    "std_dir.mkdir(parents=True, exist_ok=True)\n",
    "csv_dir.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8655b156-6920-487b-b8e7-9f7c7c87de19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i’m only renaming common codes to readable names\n",
    "rename_map = {\n",
    "    \"caseid\": \"case_id\",\n",
    "    \"v000\": \"survey_code\",\n",
    "    \"v001\": \"cluster\",\n",
    "    \"v002\": \"household\",\n",
    "    \"v003\": \"mother_line\",\n",
    "    \"v004\": \"sample_strata\",\n",
    "    \"v005\": \"weight_raw\",\n",
    "    \"v006\": \"month_interview\",\n",
    "    \"v007\": \"year_interview\",\n",
    "    \"v008\": \"century_month_code\",\n",
    "    \"b4\": \"child_sex\",\n",
    "    \"b5\": \"child_alive\",\n",
    "    \"hw1\": \"child_age_months\",\n",
    "    \"hw2\": \"child_weight_kg\",\n",
    "    \"hw3\": \"child_height_cm\",\n",
    "    \"hw70\": \"z_height_for_age\",\n",
    "    \"hw71\": \"z_weight_for_height\",\n",
    "    \"hw72\": \"z_weight_for_age\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "37ab36e6-df80-4e98-a53d-faaa7e80b54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_dta_files():\n",
    "    # look for all .dta files I already renamed like Ethiopia_KR_2016.dta, etc.\n",
    "    return sorted(unzipped.rglob(\"*.dta\"))\n",
    "\n",
    "def ensure_unique_columns(cols):\n",
    "    # if a column name repeats, i’ll add a suffix so nothing gets dropped\n",
    "    seen = {}\n",
    "    out = []\n",
    "    for c in cols:\n",
    "        if c not in seen:\n",
    "            seen[c] = 0\n",
    "            out.append(c)\n",
    "        else:\n",
    "            seen[c] += 1\n",
    "            out.append(f\"{c}__dup{seen[c]}\")\n",
    "    return out\n",
    "\n",
    "def parse_country_year_from_name(name):\n",
    "    # pulls country + year token from filename like Ethiopia_KR_2016.dta\n",
    "    m = re.match(r\"([A-Za-z]+)_KR_([0-9A-Za-z]+)\\.dta$\", name)\n",
    "    if m:\n",
    "        return m.group(1), m.group(2)\n",
    "    return \"Unknown\", \"Unknown\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "30730909-117e-4eac-a427-ff2316bca9a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found .dta files: 25\n",
      "skip existing: Ethiopia_KR_2005.dta\n",
      "skip existing: Ethiopia_KR_51.dta\n",
      "skip existing: Ethiopia_KR_2011.dta\n",
      "skip existing: Ethiopia_KR_2016.dta\n",
      "skip existing: Ethiopia_KR_2021.dta\n"
     ]
    }
   ],
   "source": [
    "dta_files = list_dta_files()\n",
    "print(\"found .dta files:\", len(dta_files))\n",
    "\n",
    "for dta in dta_files:\n",
    "    base_name = dta.name.replace(\".dta\", \"_cleaned\")\n",
    "    pq_path  = std_dir / f\"{base_name}.parquet\"\n",
    "    csv_path = csv_dir  / f\"{base_name}.csv\"\n",
    "\n",
    "    # skip if both outputs already exist and i don’t want to overwrite\n",
    "    if not OVERWRITE and pq_path.exists() and csv_path.exists():\n",
    "        print(\"skip existing:\", dta.name)\n",
    "        continue\n",
    "\n",
    "    # read the stata file with raw codes; i’m not converting categories\n",
    "    df = pd.read_stata(dta, convert_categoricals=False, preserve_dtypes=False)\n",
    "\n",
    "    # first check duplicates before rename\n",
    "if len(df.columns) != len(set(df.columns)):\n",
    "    df.columns = ensure_unique_columns(df.columns)\n",
    "\n",
    "# rename the common DHS variable codes\n",
    "df = df.rename(columns=rename_map)\n",
    "\n",
    "# re-check duplicates after renaming (some names like caseid → case_id can collide)\n",
    "if len(df.columns) != len(set(df.columns)):\n",
    "    df.columns = ensure_unique_columns(df.columns)\n",
    "\n",
    "\n",
    "    # label inside the file so i always know where each row came from\n",
    "    country, year_token = parse_country_year_from_name(dta.name)\n",
    "    df[\"file_country\"] = country\n",
    "    df[\"file_year\"] = year_token\n",
    "    df[\"file_type\"] = \"KR\"\n",
    "    df[\"original_filename\"] = dta.name\n",
    "\n",
    "    # write both formats with the exact same basename\n",
    "    df.to_parquet(pq_path, index=False)\n",
    "    df.to_csv(csv_path, index=False)\n",
    "\n",
    "    print(\"exported:\", dta.name, \"→\", pq_path.name, \"and\", csv_path.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d17e6a84-1134-42b1-86ab-4000d3e1cdbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: 25\n",
      "parquets: 5\n",
      "csvs: 5\n",
      "missing parquet for: ['Kenya_KR_1988_cleaned', 'Kenya_KR_1998_cleaned', 'Kenya_KR_1999_cleaned', 'Kenya_KR_2008_cleaned', 'Kenya_KR_2010_cleaned', 'Kenya_KR_2022_cleaned', 'Kenya_KR_72_cleaned', 'Tanzania_KR_1996_cleaned', 'Tanzania_KR_1999_cleaned', 'Tanzania_KR_2005_cleaned', 'Tanzania_KR_2007_cleaned', 'Tanzania_KR_2013_cleaned', 'Tanzania_KR_2018_cleaned', 'Tanzania_KR_2023_cleaned', 'Uganda_KR_1988_cleaned', 'Uganda_KR_2000_cleaned', 'Uganda_KR_2005_cleaned', 'Uganda_KR_2010_cleaned', 'Uganda_KR_2011_cleaned', 'Uganda_KR_2018_cleaned']\n",
      "missing csv for: ['Kenya_KR_1988_cleaned', 'Kenya_KR_1998_cleaned', 'Kenya_KR_1999_cleaned', 'Kenya_KR_2008_cleaned', 'Kenya_KR_2010_cleaned', 'Kenya_KR_2022_cleaned', 'Kenya_KR_72_cleaned', 'Tanzania_KR_1996_cleaned', 'Tanzania_KR_1999_cleaned', 'Tanzania_KR_2005_cleaned', 'Tanzania_KR_2007_cleaned', 'Tanzania_KR_2013_cleaned', 'Tanzania_KR_2018_cleaned', 'Tanzania_KR_2023_cleaned', 'Uganda_KR_1988_cleaned', 'Uganda_KR_2000_cleaned', 'Uganda_KR_2005_cleaned', 'Uganda_KR_2010_cleaned', 'Uganda_KR_2011_cleaned', 'Uganda_KR_2018_cleaned']\n"
     ]
    }
   ],
   "source": [
    "inputs = [p.name.replace(\".dta\",\"_cleaned\") for p in list_dta_files()]\n",
    "parquets = [p.name.replace(\".parquet\",\"\") for p in std_dir.glob(\"*_cleaned.parquet\")]\n",
    "csvs     = [p.name.replace(\".csv\",\"\") for p in csv_dir.glob(\"*_cleaned.csv\")]\n",
    "\n",
    "print(\"inputs:\", len(inputs))\n",
    "print(\"parquets:\", len(parquets))\n",
    "print(\"csvs:\", len(csvs))\n",
    "print(\"missing parquet for:\", sorted(set(inputs) - set(parquets)))\n",
    "print(\"missing csv for:\", sorted(set(inputs) - set(csvs)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "420af724-78cf-407d-a99c-16ee99c803f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_unique_columns(cols):\n",
    "    # if a column name repeats, i suffix it so nothing is dropped\n",
    "    seen = {}\n",
    "    out = []\n",
    "    for c in map(str, cols):  # make sure everything is a string\n",
    "        if c not in seen:\n",
    "            seen[c] = 0\n",
    "            out.append(c)\n",
    "        else:\n",
    "            seen[c] += 1\n",
    "            out.append(f\"{c}__dup{seen[c]}\")\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7411013c-bc85-4435-bd3f-6900bb78509a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total inputs: 25\n",
      "already done: 5\n",
      "still missing: 20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Kenya_KR_1988_cleaned',\n",
       " 'Kenya_KR_1998_cleaned',\n",
       " 'Kenya_KR_1999_cleaned',\n",
       " 'Kenya_KR_2008_cleaned',\n",
       " 'Kenya_KR_2010_cleaned',\n",
       " 'Kenya_KR_72_cleaned',\n",
       " 'Kenya_KR_2022_cleaned',\n",
       " 'Tanzania_KR_1996_cleaned',\n",
       " 'Tanzania_KR_1999_cleaned',\n",
       " 'Tanzania_KR_2005_cleaned']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# inputs from .dta\n",
    "inputs = [p for p in list_dta_files()]\n",
    "input_bases = [p.name.replace(\".dta\",\"_cleaned\") for p in inputs]\n",
    "\n",
    "# already-produced outputs\n",
    "parquets = [p.name.replace(\".parquet\",\"\") for p in std_dir.glob(\"*_cleaned.parquet\")]\n",
    "csvs     = [p.name.replace(\".csv\",\"\") for p in csv_dir.glob(\"*_cleaned.csv\")]\n",
    "\n",
    "have_both = set(parquets).intersection(set(csvs))\n",
    "missing_bases = [b for b in input_bases if b not in have_both]\n",
    "\n",
    "print(\"total inputs:\", len(inputs))\n",
    "print(\"already done:\", len(have_both))\n",
    "print(\"still missing:\", len(missing_bases))\n",
    "missing_bases[:10]  # preview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "aed1c5f3-2d7a-4db2-b8d4-ce3ed4f3c4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exported: Kenya_KR_1988.dta → Kenya_KR_1988_cleaned.parquet and Kenya_KR_1988_cleaned.csv\n",
      "exported: Kenya_KR_1998.dta → Kenya_KR_1998_cleaned.parquet and Kenya_KR_1998_cleaned.csv\n",
      "exported: Kenya_KR_1999.dta → Kenya_KR_1999_cleaned.parquet and Kenya_KR_1999_cleaned.csv\n",
      "exported: Kenya_KR_2008.dta → Kenya_KR_2008_cleaned.parquet and Kenya_KR_2008_cleaned.csv\n",
      "exported: Kenya_KR_2010.dta → Kenya_KR_2010_cleaned.parquet and Kenya_KR_2010_cleaned.csv\n",
      "exported: Kenya_KR_72.dta → Kenya_KR_72_cleaned.parquet and Kenya_KR_72_cleaned.csv\n",
      "exported: Kenya_KR_2022.dta → Kenya_KR_2022_cleaned.parquet and Kenya_KR_2022_cleaned.csv\n",
      "exported: Tanzania_KR_1996.dta → Tanzania_KR_1996_cleaned.parquet and Tanzania_KR_1996_cleaned.csv\n",
      "exported: Tanzania_KR_1999.dta → Tanzania_KR_1999_cleaned.parquet and Tanzania_KR_1999_cleaned.csv\n",
      "exported: Tanzania_KR_2005.dta → Tanzania_KR_2005_cleaned.parquet and Tanzania_KR_2005_cleaned.csv\n",
      "exported: Tanzania_KR_2007.dta → Tanzania_KR_2007_cleaned.parquet and Tanzania_KR_2007_cleaned.csv\n",
      "exported: Tanzania_KR_2013.dta → Tanzania_KR_2013_cleaned.parquet and Tanzania_KR_2013_cleaned.csv\n",
      "exported: Tanzania_KR_2018.dta → Tanzania_KR_2018_cleaned.parquet and Tanzania_KR_2018_cleaned.csv\n",
      "exported: Tanzania_KR_2023.dta → Tanzania_KR_2023_cleaned.parquet and Tanzania_KR_2023_cleaned.csv\n",
      "exported: Uganda_KR_1988.dta → Uganda_KR_1988_cleaned.parquet and Uganda_KR_1988_cleaned.csv\n",
      "exported: Uganda_KR_2000.dta → Uganda_KR_2000_cleaned.parquet and Uganda_KR_2000_cleaned.csv\n",
      "exported: Uganda_KR_2005.dta → Uganda_KR_2005_cleaned.parquet and Uganda_KR_2005_cleaned.csv\n",
      "exported: Uganda_KR_2010.dta → Uganda_KR_2010_cleaned.parquet and Uganda_KR_2010_cleaned.csv\n",
      "exported: Uganda_KR_2011.dta → Uganda_KR_2011_cleaned.parquet and Uganda_KR_2011_cleaned.csv\n",
      "exported: Uganda_KR_2018.dta → Uganda_KR_2018_cleaned.parquet and Uganda_KR_2018_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def parse_country_year_from_name(name):\n",
    "    m = re.match(r\"([A-Za-z]+)_KR_([0-9A-Za-z]+)\\.dta$\", name)\n",
    "    if m:\n",
    "        return m.group(1), m.group(2)\n",
    "    return \"Unknown\", \"Unknown\"\n",
    "\n",
    "log = []  # collect status per file\n",
    "\n",
    "for dta in inputs:\n",
    "    base_clean = dta.name.replace(\".dta\",\"_cleaned\")\n",
    "    if base_clean not in missing_bases:\n",
    "        # already produced both parquet and csv — skip\n",
    "        continue\n",
    "\n",
    "    pq_path  = std_dir / f\"{base_clean}.parquet\"\n",
    "    csv_path = csv_dir  / f\"{base_clean}.csv\"\n",
    "\n",
    "    try:\n",
    "        # read raw; do not convert categories\n",
    "        df = pd.read_stata(dta, convert_categoricals=False, preserve_dtypes=False)\n",
    "\n",
    "        # ensure unique BEFORE rename\n",
    "        if len(df.columns) != len(set(df.columns)):\n",
    "            df.columns = ensure_unique_columns(df.columns)\n",
    "\n",
    "        # apply your rename map (rename-only, no drops)\n",
    "        df = df.rename(columns=rename_map)\n",
    "\n",
    "        # ensure unique AFTER rename (caseid -> case_id can collide)\n",
    "        if len(df.columns) != len(set(df.columns)):\n",
    "            df.columns = ensure_unique_columns(df.columns)\n",
    "\n",
    "        # label inside\n",
    "        country, year_token = parse_country_year_from_name(dta.name)\n",
    "        df[\"file_country\"] = country\n",
    "        df[\"file_year\"] = year_token\n",
    "        df[\"file_type\"] = \"KR\"\n",
    "        df[\"original_filename\"] = dta.name\n",
    "\n",
    "        # write both formats with identical basename\n",
    "        df.to_parquet(pq_path, index=False)\n",
    "        df.to_csv(csv_path, index=False)\n",
    "\n",
    "        print(\"exported:\", dta.name, \"→\", pq_path.name, \"and\", csv_path.name)\n",
    "        log.append({\"file\": dta.name, \"status\": \"ok\", \"rows\": len(df)})\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"FAILED:\", dta.name, \"reason:\", str(e))\n",
    "        log.append({\"file\": dta.name, \"status\": \"fail\", \"reason\": str(e)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3be7150f-3156-4b23-8851-a2f43bf91f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: 25\n",
      "parquets: 25\n",
      "csvs: 25\n",
      "missing parquet for: []\n",
      "missing csv for: []\n"
     ]
    }
   ],
   "source": [
    "inputs = [p.name.replace(\".dta\",\"_cleaned\") for p in list_dta_files()]\n",
    "parquets = [p.name.replace(\".parquet\",\"\") for p in std_dir.glob(\"*_cleaned.parquet\")]\n",
    "csvs     = [p.name.replace(\".csv\",\"\") for p in csv_dir.glob(\"*_cleaned.csv\")]\n",
    "\n",
    "print(\"inputs:\", len(inputs))\n",
    "print(\"parquets:\", len(parquets))\n",
    "print(\"csvs:\", len(csvs))\n",
    "print(\"missing parquet for:\", sorted(set(inputs) - set(parquets)))\n",
    "print(\"missing csv for:\", sorted(set(inputs) - set(csvs)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56eb2710-78fe-431b-bfca-25137c8a92c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
